{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "show ROOT show VERB [students, with, 5]\n",
      "students dobj show VERB []\n",
      "with prep show VERB [name]\n",
      "name pobj with ADP [5, or, age]\n",
      "5 nummod name NOUN []\n",
      "or cc name NOUN []\n",
      "age conj name NOUN []\n",
      "not neg 5 NUM []\n",
      "5 npadvmod show VERB [not]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"0\" class=\"displacy\" width=\"1625\" height=\"487.0\" style=\"max-width: none; height: 487.0px; color: #000000; background: #ffffff; font-family: Arial\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">show</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">students</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">with</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">name</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">5</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">or</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">CCONJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">age</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">not</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">ADV</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">5</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-0\" stroke-width=\"2px\" d=\"M70,352.0 C70,264.5 210.0,264.5 210.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-0\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M210.0,354.0 L218.0,342.0 202.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-1\" stroke-width=\"2px\" d=\"M70,352.0 C70,177.0 390.0,177.0 390.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-1\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M390.0,354.0 L398.0,342.0 382.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-2\" stroke-width=\"2px\" d=\"M420,352.0 C420,264.5 560.0,264.5 560.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-2\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M560.0,354.0 L568.0,342.0 552.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-3\" stroke-width=\"2px\" d=\"M595,352.0 C595,264.5 735.0,264.5 735.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-3\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">nummod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M735.0,354.0 L743.0,342.0 727.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-4\" stroke-width=\"2px\" d=\"M595,352.0 C595,177.0 915.0,177.0 915.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-4\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">cc</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M915.0,354.0 L923.0,342.0 907.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-5\" stroke-width=\"2px\" d=\"M595,352.0 C595,89.5 1095.0,89.5 1095.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-5\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1095.0,354.0 L1103.0,342.0 1087.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-6\" stroke-width=\"2px\" d=\"M1295,352.0 C1295,264.5 1435.0,264.5 1435.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-6\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">neg</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1295,354.0 L1287,342.0 1303,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-7\" stroke-width=\"2px\" d=\"M70,352.0 C70,2.0 1450.0,2.0 1450.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-7\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">npadvmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1450.0,354.0 L1458.0,342.0 1442.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 CARDINAL\n",
      "5 CARDINAL\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy.load('en')\n",
    "doc = nlp(\"who teaches Physics for hours more than 8?\")\n",
    "temp_doc = nlp(\"list of students with age not less than 5 and enrolled in Physics\")\n",
    "# temp_doc = nlp(\"list students not enrolled in Physics and Maths\")\n",
    "temp_doc = nlp(\"list students not enrolled in Physics or NLP\")\n",
    "temp_doc = nlp(\"show students with name 5 or age not 5\")\n",
    "for token in temp_doc:\n",
    "    print(token.text, token.dep_, token.head.text, token.head.pos_,\n",
    "          [child for child in token.children])\n",
    "displacy.render(temp_doc, style='dep', jupyter=True)\n",
    "\n",
    "for ent in temp_doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_negation_condition(text, condition_params):\n",
    "    dep_list = []\n",
    "    answer = {}\n",
    "    temp = {}\n",
    "    children_list = set()\n",
    "    \n",
    "    for token in nlp(text):\n",
    "        dep_list.append((str(token),  [str(child) for child in token.children]))\n",
    "        [children_list.add(str(child)) for child in token.children]\n",
    "\n",
    "#     displacy.serve(nlp(text), style=\"dep\")\n",
    "    print(condition_params)\n",
    "    for param in condition_params:\n",
    "        const_param = param[0]\n",
    "        param_list = []\n",
    "        print(param)\n",
    "        for item in dep_list:\n",
    "            if dep_list.index(item) == param[1]:\n",
    "#             if item[0] != param:\n",
    "                param_list.append(item)\n",
    "            else:\n",
    "                param_list.append(item)\n",
    "                break\n",
    "            \n",
    "        item = param_list[-1]\n",
    "        temp = item[1]\n",
    "       \n",
    "        temp_list = param_list\n",
    "        \n",
    "        for x in temp:\n",
    "            for p in param_list:\n",
    "                print(\"p \", p)\n",
    "                if p[0] == x:\n",
    "                    print(\"found entry \", p)\n",
    "                    for y in p[1]:\n",
    "                        if y not in item[1]:\n",
    "                            item[1].append(y)\n",
    "                    print(\"param list \", param_list)\n",
    "        \n",
    "        #like enrooled would not get oany parent in that case, break -> direct make children list, check if token a child or not\n",
    "        def get_dep(it, l):\n",
    "            if it in children_list:\n",
    "                for p in temp_list:\n",
    "                    print(\"p \", p)\n",
    "                    if it in p[1]:\n",
    "                        print(\"FOUND IT I , \",p[1])\n",
    "                        for y in p[1]:\n",
    "                            if y not in item[1]:\n",
    "                                l.append(y)\n",
    "                            \n",
    "                        get_dep(p[0], l)\n",
    "                        print(l)\n",
    "                        break\n",
    "            else:\n",
    "                print(\"reached here..\")\n",
    "                \n",
    "        get_dep(item[0], item[1])\n",
    "            \n",
    "        answer[param] = param_list\n",
    "        \n",
    "    print(answer)\n",
    "    dep_list.append(answer)\n",
    "    return dep_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_negation_condition(text, condition_params):\n",
    "    dep_list = []\n",
    "    answer = {}\n",
    "    temp = {}\n",
    "    negation_attr = []\n",
    "    children_list = set()\n",
    "\n",
    "    for token in nlp(text):\n",
    "        dep_list.append((str(token),  [str(child) for child in token.children]))\n",
    "        [children_list.add(str(child)) for child in token.children]\n",
    "\n",
    "    print(\"DEPENDENCY GRAPH\")\n",
    "    print(dep_list)\n",
    "    # displacy.serve(nlp(text), style=\"dep\")\n",
    "    for param in condition_params:\n",
    "        param_list = []\n",
    "\n",
    "        for item in dep_list:\n",
    "            if dep_list.index(item) != param[1]:\n",
    "                param_list.append(item)\n",
    "            else:\n",
    "                param_list.append(item)\n",
    "                break\n",
    "\n",
    "        item = param_list[-1]\n",
    "        temp = item[1]\n",
    "\n",
    "        temp_list = param_list\n",
    "        \n",
    "        for x in temp:\n",
    "            for p in reversed(param_list):\n",
    "                print(\"p \", p)\n",
    "                if p[0] == x:\n",
    "                    print(\"FOUND IT II \", p)\n",
    "                    for y in p[1]:\n",
    "                        if y not in item[1]:\n",
    "                            item[1].append(y)\n",
    "                    print(\"param list \", param_list)\n",
    "\n",
    "        #checked for case with no parent\n",
    "        def get_dep(it, l):\n",
    "            if it in children_list:\n",
    "                for p in temp_list:\n",
    "                    print(\"p \", p)\n",
    "                    if it in p[1]:\n",
    "                        print(\"FOUND IT I , \",p[1])\n",
    "                        for y in p[1]:\n",
    "                            if y not in item[1]:\n",
    "                                l.append(y)\n",
    "\n",
    "                        get_dep(p[0], l)\n",
    "                        print(l)\n",
    "                        break\n",
    "\n",
    "        get_dep(item[0], item[1])\n",
    "                \n",
    "        answer[param] = param_list\n",
    "        \n",
    "        if 'not' in param_list[-1][1]:\n",
    "            print(param_list[-1])\n",
    "            negation_attr.append((param_list[-1][0], param[1]))\n",
    "\n",
    "        print(\"check this list \", param_list)\n",
    "        print(\"NEGATION ATTRIBUTES... \", negation_attr)\n",
    "\n",
    "    return negation_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEPENDENCY GRAPH\n",
      "[('show', ['students', 'with', '5']), ('students', []), ('with', ['name']), ('name', ['5', 'and', 'age']), ('5', []), ('and', []), ('age', []), ('not', []), ('5', ['not'])]\n",
      "p  ('show', ['students', 'with', '5'])\n",
      "FOUND IT I ,  ['students', 'with', '5']\n",
      "['students', 'with', '5']\n",
      "check this list  [('show', ['students', 'with', '5']), ('students', []), ('with', ['name']), ('name', ['5', 'and', 'age']), ('5', ['students', 'with', '5'])]\n",
      "NEGATION ATTRIBUTES...  []\n",
      "p  ('5', ['not'])\n",
      "p  ('not', [])\n",
      "FOUND IT II  ('not', [])\n",
      "param list  [('show', ['students', 'with', '5']), ('students', []), ('with', ['name']), ('name', ['5', 'and', 'age']), ('5', ['students', 'with', '5']), ('and', []), ('age', []), ('not', []), ('5', ['not'])]\n",
      "p  ('age', [])\n",
      "p  ('and', [])\n",
      "p  ('5', ['students', 'with', '5'])\n",
      "p  ('name', ['5', 'and', 'age'])\n",
      "p  ('with', ['name'])\n",
      "p  ('students', [])\n",
      "p  ('show', ['students', 'with', '5'])\n",
      "p  ('show', ['students', 'with', '5'])\n",
      "FOUND IT I ,  ['students', 'with', '5']\n",
      "['not', 'students', 'with', '5']\n",
      "('5', ['not', 'students', 'with', '5'])\n",
      "check this list  [('show', ['students', 'with', '5']), ('students', []), ('with', ['name']), ('name', ['5', 'and', 'age']), ('5', ['students', 'with', '5']), ('and', []), ('age', []), ('not', []), ('5', ['not', 'students', 'with', '5'])]\n",
      "NEGATION ATTRIBUTES...  [('5', 8)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('5', 8)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = \"show students with name 5 and age not 5\"\n",
    "get_negation_condition(txt, [('5',4), ('5', 8)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method to get pairs attribute value with position\n",
    "change code to not match list before token match but with position\n",
    "to solve pass values coz attributes can be same\n",
    "values can be same as well for different attributes, so pass position\n",
    "major-? values, position\n",
    "py method to get all indexes of a word\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "students of class 5 and age not 5\n",
      "5\n",
      "0 0\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "sentence = input(\"students of class 5 and age not 5\")\n",
    "word = input(\"5\")\n",
    "for match in re.finditer(word, sentence):\n",
    "    print (match.start(), match.end())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 7]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"students of class 5 and age not 5\"\n",
    "l = sentence.split()\n",
    "l.index('5')\n",
    "\n",
    "def duplicates(lst, item):\n",
    "    return [i for i, x in enumerate(lst) if x == item]\n",
    "duplicates(l, '5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'doc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-1e7cc187e2eb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtemp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'doc' is not defined"
     ]
    }
   ],
   "source": [
    "temp_list =  ['teaches', 'instructor', 'course']\n",
    "hash_map =  {'Physics': [['course', 'course']]}\n",
    "temp_dict = {}\n",
    "x = {}\n",
    "temp = False\n",
    "for i in x:\n",
    "    if x[i][0][1] == 'S':\n",
    "        temp = True        \n",
    "\n",
    "if not x or not temp:\n",
    "    for token in doc:\n",
    "        if token.text in temp_list:\n",
    "            continue\n",
    "            \n",
    "        if token.text in hash_map.keys():\n",
    "            if hash_map[token.text][0][0] in temp_list:\n",
    "                continue\n",
    "        \n",
    "        if token.dep_ == 'nsubj':\n",
    "            print(\"add its pk to x with select tag\", token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'who' not in hash_map.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy.load('en')\n",
    "doc1 = nlp('This is a sentence.')\n",
    "doc2 = nlp('This is another sentence.')\n",
    "displacy.serve([doc1, doc2], style='dep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy.load('en')\n",
    "doc1 = nlp(u'This is a sentence.')\n",
    "doc2 = nlp(u'This is another sentence.')\n",
    "displacy.render([doc1, doc2], style='dep', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = \"hourly_salary\"\n",
    "t2 = \"income\"\n",
    "nlp(t1).similarity(nlp(t2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp2 = spacy.load('retrained_en_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_doc = nlp2(doc)\n",
    "for ent in nlp_doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \"Michael teaches Physics\"\n",
    "doc2 = \"Alex is good at Chemistry but scored 20 in Biology\"\n",
    "doc3 = \"What is the strength in Philosophy class\"\n",
    "doc4 = \"How many students are enrolled for Parrallel Computing 101\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ent(nlp_doc):\n",
    "    for ent in nlp_doc.ents:\n",
    "        print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ent(nlp2(doc))\n",
    "get_ent(nlp2(doc2))\n",
    "get_ent(nlp2(doc3))\n",
    "get_ent(nlp2(doc4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc5 = \"Find the teachers with salary greater than 20000 but teaching hours not less than 8 hours\"\n",
    "doc10 = \"Select teachers whose age is not 50\"\n",
    "doc11 = \"Show teachers whose salary is not e 30000$\"\n",
    "for token in nlp(doc11):\n",
    "    print(token.text, token.dep_, token.head.text, token.head.pos_,\n",
    "          [child for child in token.children])\n",
    "displacy.render(nlp(doc5), style='dep', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PHRASE MATCHER\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "matcher.add(\"OBAMA\", None, nlp(u\"Barack Obama\"))\n",
    "doc = nlp(u\"Barack Obama lifts America one last time in emotional farewell\")\n",
    "matches = matcher(doc)\n",
    "for ent_id, start, end in matches:\n",
    "    print(ent_id, doc[start:end].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import itertools\n",
    "import logging\n",
    "\n",
    "from spacy.symbols import NOUN, PROPN, VERB\n",
    "from spacy.tokens.token import Token as SpacyToken\n",
    "from spacy.tokens.span import Span as SpacySpan\n",
    "\n",
    "from . import constants\n",
    "from . import text_utils\n",
    "from . import utils\n",
    "\n",
    "LOGGER = logging.getLogger(__name__)\n",
    "\n",
    "utils.deprecated(\n",
    "    \"The `spacy_utils` module is deprecated and will be removed in v0.7.0.\"\n",
    "    \"Use the `textacy.spacier` subpackage instead.\",\n",
    "    action=\"once\",\n",
    ")\n",
    "\n",
    "def is_plural_noun(token):\n",
    "    \"\"\"\n",
    "    Returns True if token is a plural noun, False otherwise.\n",
    "\n",
    "    Args:\n",
    "        token (``spacy.Token``): parent document must have POS information\n",
    "\n",
    "    Returns:\n",
    "        bool\n",
    "    \"\"\"\n",
    "    if token.doc.is_tagged is False:\n",
    "        raise ValueError(\"token is not POS-tagged\")\n",
    "    if token.pos == NOUN and token.lemma != token.lower:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "def is_negated_verb(token):\n",
    "    \"\"\"\n",
    "    Returns True if verb is negated by one of its (dependency parse) children,\n",
    "    False otherwise.\n",
    "\n",
    "    Args:\n",
    "        token (``spacy.Token``): parent document must have parse information\n",
    "\n",
    "    Returns:\n",
    "        bool\n",
    "\n",
    "    TODO: generalize to other parts of speech; rule-based is pretty lacking,\n",
    "    so will probably require training a model; this is an unsolved research problem\n",
    "    \"\"\"\n",
    "    if token.doc.is_parsed is False:\n",
    "        raise ValueError(\"token is not parsed\")\n",
    "    if token.pos == VERB and any(c.dep_ == \"neg\" for c in token.children):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def preserve_case(token):\n",
    "    \"\"\"\n",
    "    Returns True if `token` is a proper noun or acronym, False otherwise.\n",
    "\n",
    "    Args:\n",
    "        token (``spacy.Token``): parent document must have POS information\n",
    "\n",
    "    Returns:\n",
    "        bool\n",
    "    \"\"\"\n",
    "    if token.pos == PROPN or text_utils.is_acronym(token.text):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "def normalized_str(token):\n",
    "    \"\"\"\n",
    "    Return as-is text for tokens that are proper nouns or acronyms, lemmatized\n",
    "    text for everything else.\n",
    "\n",
    "    Args:\n",
    "        token (``spacy.Token`` or ``spacy.Span``)\n",
    "\n",
    "    Returns:\n",
    "        str\n",
    "    \"\"\"\n",
    "    if isinstance(token, SpacyToken):\n",
    "        return token.text if preserve_case(token) else token.lemma_\n",
    "    elif isinstance(token, SpacySpan):\n",
    "        return \" \".join(\n",
    "            subtok.text if preserve_case(subtok) else subtok.lemma_ for subtok in token\n",
    "        )\n",
    "    else:\n",
    "        raise TypeError(\n",
    "            \"Input must be a spacy Token or Span, not {}.\".format(type(token))\n",
    "        )\n",
    "\n",
    "\n",
    "def merge_spans(spans):\n",
    "    \"\"\"\n",
    "    Merge spans *in-place* within parent doc so that each takes up a single token.\n",
    "\n",
    "    Args:\n",
    "        spans (Iterable[``spacy.Span``])\n",
    "    \"\"\"\n",
    "    for span in spans:\n",
    "        try:\n",
    "            span.merge(span.root.tag_, span.text, span.root.ent_type_)\n",
    "        except IndexError as e:\n",
    "            LOGGER.exception('Unable to merge span \"%s\"; skipping...', span.text)\n",
    "\n",
    "\n",
    "\n",
    "def get_main_verbs_of_sent(sent):\n",
    "    \"\"\"Return the main (non-auxiliary) verbs in a sentence.\"\"\"\n",
    "    return [\n",
    "        tok for tok in sent if tok.pos == VERB and tok.dep_ not in {\"aux\", \"auxpass\"}\n",
    "    ]\n",
    "\n",
    "\n",
    "def get_subjects_of_verb(verb):\n",
    "    \"\"\"Return all subjects of a verb according to the dependency parse.\"\"\"\n",
    "    subjs = [tok for tok in verb.lefts if tok.dep_ in constants.SUBJ_DEPS]\n",
    "    # get additional conjunct subjects\n",
    "    subjs.extend(tok for subj in subjs for tok in _get_conjuncts(subj))\n",
    "    return subjs\n",
    "\n",
    "\n",
    "\n",
    "def get_objects_of_verb(verb):\n",
    "    \"\"\"\n",
    "    Return all objects of a verb according to the dependency parse,\n",
    "    including open clausal complements.\n",
    "    \"\"\"\n",
    "    objs = [tok for tok in verb.rights if tok.dep_ in constants.OBJ_DEPS]\n",
    "    # get open clausal complements (xcomp)\n",
    "    objs.extend(tok for tok in verb.rights if tok.dep_ == \"xcomp\")\n",
    "    # get additional conjunct objects\n",
    "    objs.extend(tok for obj in objs for tok in _get_conjuncts(obj))\n",
    "    return objs\n",
    "\n",
    "\n",
    "\n",
    "def _get_conjuncts(tok):\n",
    "    \"\"\"\n",
    "    Return conjunct dependents of the leftmost conjunct in a coordinated phrase,\n",
    "    e.g. \"Burton, [Dan], and [Josh] ...\".\n",
    "    \"\"\"\n",
    "    return [right for right in tok.rights if right.dep_ == \"conj\"]\n",
    "\n",
    "\n",
    "def get_span_for_compound_noun(noun):\n",
    "    \"\"\"\n",
    "    Return document indexes spanning all (adjacent) tokens\n",
    "    in a compound noun.\n",
    "    \"\"\"\n",
    "    min_i = noun.i - sum(\n",
    "        1\n",
    "        for _ in itertools.takewhile(\n",
    "            lambda x: x.dep_ == \"compound\", reversed(list(noun.lefts))\n",
    "        )\n",
    "    )\n",
    "    return (min_i, noun.i)\n",
    "\n",
    "\n",
    "\n",
    "def get_span_for_verb_auxiliaries(verb):\n",
    "    \"\"\"\n",
    "    Return document indexes spanning all (adjacent) tokens\n",
    "    around a verb that are auxiliary verbs or negations.\n",
    "    \"\"\"\n",
    "    min_i = verb.i - sum(\n",
    "        1\n",
    "        for _ in itertools.takewhile(\n",
    "            lambda x: x.dep_ in constants.AUX_DEPS, reversed(list(verb.lefts))\n",
    "        )\n",
    "    )\n",
    "    max_i = verb.i + sum(\n",
    "        1\n",
    "        for _ in itertools.takewhile(\n",
    "            lambda x: x.dep_ in constants.AUX_DEPS, verb.rights\n",
    "        )\n",
    "    )\n",
    "    return (min_i, max_i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_negated_verb(token):\n",
    "    \"\"\"\n",
    "    Returns True if verb is negated by one of its (dependency parse) children,\n",
    "    False otherwise.\n",
    "\n",
    "    Args:\n",
    "        token (``spacy.Token``): parent document must have parse information\n",
    "\n",
    "    Returns:\n",
    "        bool\n",
    "\n",
    "    TODO: generalize to other parts of speech; rule-based is pretty lacking,\n",
    "    so will probably require training a model; this is an unsolved research problem\n",
    "    \"\"\"\n",
    "    if token.doc.is_parsed is False:\n",
    "        raise ValueError(\"token is not parsed\")\n",
    "    if token.pos == VERB and any(c.dep_ == \"neg\" for c in token.children):\n",
    "        return True\n",
    "    else:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc6 = \"teachers with age not less than 40\"\n",
    "for token in nlp(doc6):\n",
    "    print(token, token.pos_,is_negated_verb(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc7 = \"this camera is not pretty\"\n",
    "for token in nlp(doc7):\n",
    "    print(token, token.pos_,is_negated_verb(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "less = wn.synset('less.a.01')\n",
    "print(less)\n",
    "# Synset('able.a.01')\n",
    " \n",
    "print(less.lemmas())\n",
    "# [Lemma('able.a.01.able')]\n",
    " \n",
    "print(less.lemmas()[0].antonyms())\n",
    "# [Lemma('unable.a.01.unable')]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "less = wn.synset('overshoot.v.01')\n",
    "print(less)\n",
    "# Synset('able.a.01')\n",
    " \n",
    "print(less.lemmas())\n",
    "# [Lemma('able.a.01.able')]\n",
    " \n",
    "print(less.lemmas()[0].antonyms())\n",
    "# [Lemma('unable.a.01.unable')]\n",
    " \n",
    "print(less.hypernyms())\n",
    "\n",
    "print(less.hyponyms())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_test = \"the movie wasn't that good\"\n",
    "for token in nlp(doc_test):\n",
    "    print(token, token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oov = nlp.vocab.strings[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oov in nlp.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str = \"studentid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp('salary').similarity(nlp('income'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp('income')\n",
    "for token in doc:\n",
    "    print(token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = {}\n",
    "s['income'] = 'S'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s['salary'] = s['income']\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del s['income']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.append(1)\n",
    "a.append(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if b:\n",
    "    if 2 > max(b):\n",
    "        b.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.append(('salary', 9.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.append(('znc', 9.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(b,key=lambda item:item[1])[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = 'Gujarat'\n",
    "nldoc = nlp(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in nldoc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for synset in list(wn.all_synsets('n'))[:10]:\n",
    "    print(synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.synsets('address')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = wn.synset('least.n.01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "types_of_cats = g.hyponyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for synset in types_of_cats:\n",
    "    for lemma in synset.lemmas():\n",
    "        print(lemma.name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "types_of_dog = g.hypernyms()\n",
    "for synset in types_of_dog:\n",
    "    for lemma in synset.lemmas():\n",
    "        print(lemma.name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.synset('address.n.01').path_similarity(wn.synset('live.v.01'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.morphy('address')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ss in wn.synsets('gujarat'):\n",
    "    print(ss.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = wn.synset('english.n.01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[lemma.name() for synset in loc.hypernyms() for lemma in synset.lemmas()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.synset('jabalpur.n.01').part_holonyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.synset('live.v.01').part_holonyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.synset('gujarat.n.01').part_meronyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.synset('gujarat.n.01').entailments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.synsets('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "any(b,key=lambda item:item[1])[1] < 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \"list students with age atleast 5\"\n",
    "for token in nlp(doc):\n",
    "    print(token, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    " im = {'Mathematics': [], 'Physics': [['course', 'course']]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[['course', 'course']]\n"
     ]
    }
   ],
   "source": [
    "for i in im:\n",
    "    print(im[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
